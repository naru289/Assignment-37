{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-37/blob/main/M4_AST_37_Momentum_Contrast(MoCo)_Cifar10_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_bf42yi38mv"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment: Momentum Contrast (MoCo) for Unsupervised Visual Representation Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* implement Momentum Contrast(MoCo) for Unsupervised Visual Representation Learning"
      ],
      "metadata": {
        "id": "bdTCiU5vipLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "gDaBSV4WiTpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description"
      ],
      "metadata": {
        "id": "jkniXT1bmDdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "The dataset is divided into five training batches and one test batch where each batch has 10000 images. The test batch contains 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n",
        "\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
      ],
      "metadata": {
        "id": "pf_glcYXmAnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information"
      ],
      "metadata": {
        "id": "gP8StRLcmrfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised representation learning is highly successful in natural language processing, e.g., as shown by BERT. But supervised pre-training is still dominant in computer vision, where unsupervised methods generally lag behind. The reason may stem from differences in their respective signal spaces. Language tasks have discrete signal spaces (words, sub-word units, etc.) for building tokenized dictionaries, on which unsupervised\n",
        "learning can be based. Computer vision, in contrast, further\n",
        "concerns dictionary building, as the raw signal is\n",
        "in a continuous, high-dimensional space and is not structured for human communication (e.g., unlike words)\n",
        "\n",
        "Several recent studies present promising results on unsupervised visual representation learning using approaches related to the contrastive loss. Though driven by various motivations, these methods\n",
        "can be thought of as building **dynamic dictionaries**. The\n",
        "**“keys”** (tokens) in the dictionary are sampled from data\n",
        "(e.g., images or patches) and are represented by an encoder\n",
        "network. Unsupervised learning trains encoders to perform\n",
        "dictionary look-up: an encoded **“query”** should be similar\n",
        "to its matching key and dissimilar to others. Learning is\n",
        "formulated as minimizing a contrastive loss.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1016/1*AURmxepRI4G6WT0DirxZDw.png\" width=450px/>\n",
        "</center>\n",
        "\n",
        "Momentum Contrast (MoCo) trains a visual represen-\n",
        "tation encoder by matching an encoded **query q** to a dictionary\n",
        "of encoded keys using a contrastive loss. The dictionary keys\n",
        "{k0, k1, k2, ...} are defined by a set of data samples.\n",
        "The dictionary is built as a queue, with the current mini-batch enqueued and the oldest mini-batch dequeued, decoupling it from\n",
        "the mini-batch size. The keys are encoded by a slowly progressing\n",
        "encoder, driven by a momentum update with the query encoder.\n",
        "This method enables a large and consistent dictionary for learning\n",
        "visual representations.\n",
        "\n",
        "From this perspective, we hypothesize that it is desirable\n",
        "to build dictionaries that are: (i) large and (ii) consistent\n",
        "as they evolve during training. Intuitively, a larger dictionary may better sample the underlying continuous, highdimensional visual space, while the keys in the dictionary should be represented by the same or similar encoder so that their comparisons to the query are consistent.\n",
        "\n",
        "\n",
        "We present **Momentum Contrast (MoCo)** as a way of building large and consistent dictionaries for unsupervised learning with a contrastive loss (figure above). We maintain the **dictionary as a queue** of data samples: the encoded representations of the current mini-batch are enqueued, and the\n",
        "oldest are dequeued. The queue decouples the dictionary\n",
        "size from the mini-batch size, allowing it to be large. Moreover, as the dictionary keys come from the preceding several mini-batches, a slowly progressing key encoder, implemented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.\n",
        "\n",
        "\n",
        "In the [paper](https://arxiv.org/pdf/1911.05722.pdf), they followed a simple instance discrimination task: a query matches a key if they are encoded views (e.g., different crops) of the same image. Using this pretext task, MoCo shows competitive results.\n",
        "\n",
        "**Contrastive learning** is used for unsupervised pre-training. Contrastive learning is to learn a metric space between two samples (images in our case) in which the distance between two positive samples is reduced while the distance between two negative samples is enlarged. Positive samples can be represented by samples from the same category or different augmented versions of the same sample, while negative samples can be represented by samples from different categories.\n",
        "\n",
        "A main purpose of unsupervised learning is to pre-train\n",
        "representations (i.e., features) that can be transferred to\n",
        "downstream tasks by fine-tuning. These results show that MoCo largely closes the gap between unsupervised and supervised representation learning in many\n",
        "computer vision tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "uoJLWjykmyom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MoCo (Momentum Contrast)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The training process is designed as follows:\n",
        "\n",
        "* A query image is selected and processed by the encoder network to compute q, the encoded query image.\n",
        "\n",
        "* Since the goal of the model is learn to differentiate between a large number of different images, this query image encoding is not only compared to one mini-batch of encoded key images, but to multiple of them.\n",
        "\n",
        "* To achieve that, MoCo forms a queue of mini-batches that are encoded by the momentum encoder network. As a new mini-batch is selected, its encodings are enqueued and the oldest encodings in the data structure are dequeued. This decouples the dictionary size, represented by the queue, from the batch size and enables a much larger dictionary to query from.\n",
        "\n",
        "If the encoding of the query image matches a key in the dictionary, these two views are deemed to be from the same image (e.g. multiple different crops).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P6KOSFf__lNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "These are the ResNet-18 classification accuracy of a **kNN monitor** on the unsupervised pre-training features.\n",
        "\n",
        "| config | 200ep | 400ep | 800ep |\n",
        "| --- | --- | --- | --- |\n",
        "| Asymmetric | 82.6 | 86.3 | 88.7 |\n",
        "| Symmetric | 85.3 | 88.5 | 89.7 |\n",
        "\n",
        "#### Notes\n",
        "\n",
        "* **Symmetric loss**: the original MoCo [paper](https://arxiv.org/pdf/1911.05722.pdf) uses an *asymmetric* loss -- one crop is the query and the other crop is the key, and it backpropagates to one crop (query). Following SimCLR/BYOL, here we provide an option of a *symmetric* loss -- it swaps the two crops and computes an extra loss. The symmetric loss behaves like 2x epochs of the asymmetric counterpart: this may dominate the comparison results when the models are trained with a fixed epoch number.\n",
        "\n",
        "* **SplitBatchNorm**: the original MoCo was trained in 8 GPUs. To simulate the multi-GPU behavior of BatchNorm in this 1-GPU, we provide a SplitBatchNorm layer. We set `bn_splits 8` by default to simulate 8 GPUs. `bn_splits 1` is analogous to SyncBatchNorm in the multi-GPU case.\n",
        "\n",
        "* **kNN monitor**: The paper provides a kNN monitor on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "5-_0Vd4edR_s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vInRrxGKIu64",
        "outputId": "8dc9ab50-fa26-455e-a443-9c78c5fb5ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_37_Momentum_Contrast(MoCo)_Cifar10_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/MoCo_model_checkpoint.pth\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=3022\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MXcbCW8f7Eh"
      },
      "source": [
        "### Importing required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvAFrPlS4bLU"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from PIL import Image\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing CUDA\n",
        "\n",
        "CUDA is used as an interface between our code and the GPU.\n",
        "\n",
        "Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"
      ],
      "metadata": {
        "id": "2YSVI2EAhReW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To test whether GPU instance is present in the system of not.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"
      ],
      "metadata": {
        "id": "2obSekgKhStL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it's False, then we run the program on CPU. If it's True, then we run the program on GPU.\n",
        "\n",
        "Let us initialize some GPU-related variables:"
      ],
      "metadata": {
        "id": "AMB1uRakhWfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "tNb6uo5rhYiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Cifar-10 dataset\n"
      ],
      "metadata": {
        "id": "VGzrG6WYhjht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "# The data augmentation setting proposed by the paper\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),    # A 32x32 pixel crop is taken from a randomly resized image\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # Random horizontal flip\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8), # Random color jittering\n",
        "    transforms.RandomGrayscale(p=0.2),   # Random grayscale conversion (Color transformation involves basic adjustments of color levels in an image)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])]) # Normalizing with mean and standard deviation values\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"
      ],
      "metadata": {
        "id": "hAI8YDZ2gkfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below class takes the CIFAR-10 dataset and return the image pairs along with applied transformations."
      ],
      "metadata": {
        "id": "UTGVfWoijvTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Pair(CIFAR10):\n",
        "    \"\"\"CIFAR10 Dataset.\n",
        "    \"\"\"\n",
        "    def __getitem__(self, index):\n",
        "        # Select the image index\n",
        "        img = self.data[index]\n",
        "\n",
        "        # Creating image object of above array\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        # Applying transformations\n",
        "        if self.transform is not None:\n",
        "            im_1 = self.transform(img)\n",
        "            im_2 = self.transform(img)\n",
        "\n",
        "        return im_1, im_2 # Returns image pairs"
      ],
      "metadata": {
        "id": "4dDsOx9cgg8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading train and test sets\n",
        "train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
        "memory_data = CIFAR10(root='data', train=True, transform=test_transform, download=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)"
      ],
      "metadata": {
        "id": "SuomCQLgnSr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of training and test images\n",
        "dataset_sizes = {'Train': len(train_data), 'Test': len(test_data)}\n",
        "dataset_sizes"
      ],
      "metadata": {
        "id": "87fVQiy8UpaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features.\n",
        "\n",
        "1. Batching the data\n",
        "2. Shuffling the data\n",
        "3. Load the data in parallel using multiprocessing workers.\n",
        "\n",
        "\n",
        "The batches of train and test data are provided via data loaders that provide iterators over the datasets to train our models."
      ],
      "metadata": {
        "id": "lkqzF-e0nOyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoliFX5AnBJ0"
      },
      "outputs": [],
      "source": [
        "# Define dataloaders\n",
        "batch_size = 512\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
        "\n",
        "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a batch of 10 images and labels\n",
        "train_images, train_labels = next(iter(memory_loader))\n",
        "train_images.shape, train_labels.shape"
      ],
      "metadata": {
        "id": "ay_b4ZZEW5SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels Translator\n",
        "label_names = {v: k for k, v in train_data.class_to_idx.items()}\n",
        "label_names"
      ],
      "metadata": {
        "id": "IOI6Q-VsW9hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "6rfbzA-fRnt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grid of images along with their corresponding labels\n",
        "L = 3\n",
        "W = 3\n",
        "\n",
        "fig, axes = plt.subplots(L, W, figsize = (12, 12))\n",
        "axes = axes.reshape(-1)\n",
        "\n",
        "for i in np.arange(0, L*W):\n",
        "    train_images = np.clip(train_images, 0, 1)\n",
        "    axes[i].imshow(train_images[i].permute(1, 2, 0))\n",
        "    axes[i].set_title(label_names[train_labels[i].item()])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "XRIn948QXGAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Batch-Normalization\n",
        "\n",
        "Recent work has shown that using unlabeled data in selfsupervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification on the CIFAR-10 and the ImageNet datasets, and with many other forms of domain shifts applied. Our\n",
        "main contribution is Split Batch Normalization (Split-BN), a technique\n",
        "to improve SSL when the additional unlabeled data comes from a shifted\n",
        "distribution. We achieve it by using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a\n",
        "standard practice. Finally, we analyse how domain shift affects the SSL\n",
        "training process. In particular, we find that during training the statistics\n",
        "of hidden activations in late layers become markedly different between\n",
        "the unlabeled and the labeled examples.\n",
        "\n",
        "\n",
        "**Batch Normalization:** The main idea behind batch normalization is to normalize the distribution of hidden activations $h$ based on the batch statistics as follows:\n",
        "\n",
        "<center>\n",
        "$\\hat{h} = \\alpha \\frac{h - \\mu(h)}{\\sigma(h)} + \\beta$\n",
        "</center>\n",
        "\n",
        "where $\\alpha$ and $\\beta$ are learnable parameters, and $\\mu(h)$ and $\\sigma(h)$ are the mean and the standard deviation computed on the given batch $h$, called batch normalization statistics. Batch normalization leads to large improvements in both convergence speed and generalization performance of deep neural networks.\n",
        "\n",
        "**Split Batch Normalization:**\n",
        "\n",
        "Typically, during the inference batch normalization statistics are computed on\n",
        "the whole training dataset. However, these statistics are not accurate if the deep network is applied to examples coming from a different distribution. One possible solution to this issue is to recompute the statistics on the new dataset and allow the model to learn new $\\alpha$ and $\\beta$ parameters.\n",
        "\n",
        "The authors main contribution is introducing a related technique to self-supervised learning. We propose to compute separately batch normalization statistics for the unsupervised and supervised dataset. By ensuring the hidden activations have the same statistics regardless of the label presence, we aim to reduce the negative effect of a domain shift between the labeled and unlabeled examples. We will refer to this technique as **Split Batch Normalization (Split-BN)**.\n",
        "\n",
        "More precisely, let $h_l$ and $h_u$ denote the labeled and the unlabeled examples in a given batch $h$, respectively. Then Split-BN normalizes the hidden activations\n",
        "as follows:\n",
        "\n",
        "<center>\n",
        "$\\hat{h_u} = \\alpha \\frac{h_u - \\mu(h_u)}{\\sigma(h_u)} + \\beta$\n",
        "</center>\n",
        "<br>\n",
        "<center>\n",
        "$\\hat{h_l} = \\alpha \\frac{h_l - \\mu(h_l)}{\\sigma(h_l)} + \\beta$\n",
        "</center>\n",
        "\n",
        "Analogously, during the inference means and standard deviations are computed separately on the labeled, and the unlabeled examples. Even though the\n",
        "statistics are computed independently, the $\\alpha$ and $\\beta$ parameters are shared.\n",
        "\n",
        "**Note:** To understand more about split batch normalization refer to the following [link](https://arxiv.org/pdf/1904.03515.pdf).\n",
        "\n"
      ],
      "metadata": {
        "id": "i3WkmAkCStc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SplitBatchNorm: simulate multi-gpu behavior of BatchNorm in one gpu by splitting alone the batch dimension\n",
        "# implementation adapted from https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n",
        "# Redefining the nn.BatchNorm2d method with the num of batch splits\n",
        "# Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n",
        "class SplitBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "\n",
        "    # The mean and standard-deviation are calculated per-dimension over the mini-batches and α and β\n",
        "    # are learnable parameter vectors of size C (where C is the input size). By default, the elements of α are\n",
        "    # set to 1 and the elements of β are set to 0.\n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        # Also by default, during training this layer keeps running estimates of its computed mean and variance,\n",
        "        # which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1\n",
        "        # If track_running_stats is set to False, this layer then does not keep running estimates,\n",
        "        # and batch statistics are instead used during evaluation time as well.\n",
        "        if self.training or not self.track_running_stats:\n",
        "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
        "            running_var_split = self.running_var.repeat(self.num_splits)\n",
        "            outcome = nn.functional.batch_norm(\n",
        "                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split,\n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W)\n",
        "            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
        "            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
        "            return outcome\n",
        "        else:\n",
        "            return nn.functional.batch_norm(\n",
        "                input, self.running_mean, self.running_var,\n",
        "                self.weight, self.bias, False, self.momentum, self.eps)"
      ],
      "metadata": {
        "id": "-GWMlLFu1m4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsAVAtRoiBbG"
      },
      "source": [
        "### Define base ResNet 18 Encoder\n",
        "\n",
        "ResNet is a Convolutional Neural Network (CNN) architecture, made up of series of residual blocks (ResBlocks) described below with skip connections differentiating ResNets from other CNNs.\n",
        "\n",
        "We adopt a ResNet as the encoder, whose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D). This output vector is normalized. This is the representation of the query or key.\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture.png\" width=750px/>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the pipeline from images to representations:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/700/1*0bYRv7XBQPpbnxMjO7i1RQ.jpeg\" width=750px/>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "4NdCi6byNUBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNd_Q_Osi0SO"
      },
      "outputs": [],
      "source": [
        "# Create a ResNet backbone and remove the classification head\n",
        "class ModelBase(nn.Module):\n",
        "    \"\"\"\n",
        "    Common CIFAR ResNet recipe.\n",
        "    Comparing with ImageNet ResNet recipe, it:\n",
        "    (i) replaces conv1 with kernel=3, str=1\n",
        "    (ii) removes pool1\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
        "        super(ModelBase, self).__init__()\n",
        "\n",
        "        # use split batchnorm\n",
        "        norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
        "\n",
        "        # Loading the ResNet-18 architecure without pretrained weights\n",
        "        resnet_arch = getattr(resnet, arch)\n",
        "\n",
        "        # Feature representation of images are passed as number of classes to the linear layer\n",
        "        # num_classes is the output fc dimension\n",
        "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
        "\n",
        "        self.net = []\n",
        "        # This module is composed of “children” or “submodules” that define the layers of the neural network\n",
        "        # and are utilized for computation within the module’s forward() method.\n",
        "        # Immediate children of a module can be iterated through via a call to children() or named_children():\n",
        "        for name, module in net.named_children():\n",
        "            # Changing the first convolutional layer kernel size, stride and padding, remove maxpooling and adding the layers back to the module\n",
        "            if name == 'conv1':\n",
        "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            if isinstance(module, nn.MaxPool2d):\n",
        "                continue\n",
        "            if isinstance(module, nn.Linear):\n",
        "                self.net.append(nn.Flatten(1))\n",
        "            self.net.append(module)  # modules() and named_modules() recursively iterate through a module and its child modules\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        # note: not normalized here\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJFOHlOBpLay"
      },
      "source": [
        "### Define Momentum Contrast (MoCo) wrapper\n",
        "\n",
        "**Contrastive Learning as Dictionary Look-up**\n",
        "\n",
        "Contrastive learning is a way of building a discrete dictionary on high-dimensional continuous inputs such as images. The dictionary is dynamic in the sense that the keys are randomly sampled, and that the key encoder evolves during training.\n",
        "\n",
        "Contrastive learning, and its recent developments, can be thought of as training an encoder for a dictionary look-up task.\n",
        "\n",
        "Consider an encoded **query q** and a set of encoded samples **{k0, k1, k2, …}** that are the keys of a dictionary. Assume that there is a single key (denoted as k+) in the dictionary that q matches. A contrastive loss is a function whose value is low (minimizing) when q is similar to its **positive key k+** and dissimilar to all other keys (considered negative keys for q).\n",
        "With similarity measured by dot product, a form of\n",
        "a contrastive loss function, called **InfoNCE (Noise-contrastive estimation)**, is considered in the [paper](https://arxiv.org/pdf/1911.05722.pdf):\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "$L_q = -log \\frac{exp(q.k_{+}/T)}{\\sum_{i=0}^{k} exp(q.k_{i}/T)}$\n",
        "</center>\n",
        "\n",
        "\n",
        "**Dictionary as a queue (Dynamic Dictionaries):** At the core of our approach is maintaining the dictionary as a queue of data samples. This allows us to reuse the encoded keys from the immediate preceding mini-batches. The introduction of a queue decouples\n",
        "the dictionary size from the mini-batch size. Our dictionary size can be much larger than a typical mini-batch size, and can be flexibly and independently set as a hyper-parameter.\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/700/1*wvWN9acS5AlXMM0nKghRvg.png\" width=750px/>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "We can look at the contrastive learning approach in a slightly different way i.e., matching queries to keys. Instead of having a single encoder, we now have two encoders — one for query and another one for the key. Moreover, to have a large number of negative samples, we have a large dictionary of encoded keys.\n",
        "\n",
        "A **positive pair** in this context means that the query matches the key. They match if both the query and the key come from the same image. An encoded query should be similar to its matching key and dissimilar to others.\n",
        "\n",
        "For **negative pairs**, **we maintain a large dictionary which contains encoded keys from previous batches**. They serve as negative samples to the query at hand. We maintain the dictionary in the form of a **queue**. The **latest batch is enqueued and the oldest batch is dequeued**. By changing the size of this queue, change the number of negative samples.\n",
        "\n",
        "**Challenges with this approach**\n",
        "\n",
        "* Using a queue can make the dictionary large as the key encoder changes, the keys which are enqueued at later points of time can become inconsistent with the keys that were enqueued quite early. **For the contrastive learning approach to work, all the keys that are compared to the queries must come from the same or similar encoders for the comparisons to be meaningful and consistent.**\n",
        "    \n",
        "* Another challenge is that **it’s not feasible to learn the key encoder parameters using backpropagation because that would require calculating gradients for all the samples in the queue** (which would result in a large computational graph).\n",
        "\n",
        "\n",
        "\n",
        "**Momentum update:**\n",
        "\n",
        "To address both of these above issues, MoCo implements the key encoder as a momentum-based moving average of the query encoder.It means that it updates the key encoder parameters in the following way\n",
        "\n",
        "$f_k$ - Key Encoder\n",
        "<br>\n",
        "$f_q$ - Query Encoder\n",
        "\n",
        "Formally, denoting the parameters of $f_k$ as $\\theta_k$ and those\n",
        "of $f_q$ as $\\theta_q$, we update $\\theta_k$ by:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "$\\theta_k ← m\\theta_k + (1 − m) \\theta_q$\n",
        "</center>\n",
        "<br>\n",
        "Here $m ∈ [0, 1]$ is a momentum coefficient. Only the parameters $\\theta_q$ are updated by back-propagation. The momentum update in the Equation makes $\\theta_k$ evolve more smoothly than $\\theta_q$. As a result, though the keys in the queue are encoded by different encoders (in different mini-batches), the difference among these encoders can be made small. In experiments, a relatively large momentum (e.g., m = 0.999, our default) works much better than a smaller value (e.g.,m = 0.9), suggesting that a slowly evolving key encoder is\n",
        "a core to making use of a queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "lzFyFnhbk8hj"
      },
      "outputs": [],
      "source": [
        " # Build a MoCo model with: a query encoder, a key encoder, and a queue https://arxiv.org/abs/1911.05722\n",
        "class ModelMoCo(nn.Module):\n",
        "    def __init__(self, dim=128, K=4096, m=0.99, T=0.1, arch='resnet18', bn_splits=8, symmetric=True):\n",
        "        super(ModelMoCo, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 128)\n",
        "        K: queue size; number of negative keys (default: 4096)\n",
        "        m: moco momentum of updating key encoder (default: 0.99)\n",
        "        T: softmax temperature (default: 0.07)\n",
        "        \"\"\"\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "        self.symmetric = symmetric\n",
        "\n",
        "        # Create the encoders for query and key\n",
        "        # f_q, f_k: encoder networks for query and key\n",
        "        # queue: dictionary as a queue of K keys (CxK)\n",
        "        # num_classes is the output fc dimension\n",
        "        self.encoder_q = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
        "        self.encoder_k = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
        "\n",
        "        # Initialize the key encoder to have the same values as query encoder\n",
        "        # Do not update the key encoder via gradient\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)  # initialize\n",
        "            param_k.requires_grad = False  # The gradients of keys will not get updated\n",
        "\n",
        "        # create the queue\n",
        "        # For the current mini-batch, we encode the\n",
        "        # queries and their corresponding keys, which form the positive sample pairs. The negative samples are from the queue.\n",
        "        # Adds a buffer to the module\n",
        "        # First parameter is name of the buffer. The buffer can be accessed from this module using the given name\n",
        "        # second parameter is the operations that run on buffers\n",
        "        # Create the queue to store negative samples\n",
        "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
        "\n",
        "        # Normalization of the negative keys in a specified dimensions\n",
        "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
        "\n",
        "        # Initialize Queue pointer (dequeue and enqueue)\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        # For each of the parameters in each encoder\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            # θk ← m.θk + (1 − m)θq.\n",
        "            # Only the parameters θq are updated by back-propagation.\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, keys):\n",
        "        '''\n",
        "        Update the memory / queue.\n",
        "        Add batch to end of most recent sample index and remove the oldest samples in the queue.\n",
        "        Store location of most recent sample index (ptr).\n",
        "        args:\n",
        "            feat_k (Tensor): Feature reprentations of the view x_k computed by the key_encoder.\n",
        "        '''\n",
        "\n",
        "        # gather keys before updating queue\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0  # for simplicity\n",
        "\n",
        "        # replace the keys at ptr (dequeue and enqueue)\n",
        "        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n",
        "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
        "\n",
        "        self.queue_ptr[0] = ptr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _batch_shuffle_single_gpu(self, x):\n",
        "        \"\"\"\n",
        "        Batch shuffle, for making use of splitBatchNorm.\n",
        "        \"\"\"\n",
        "        # random shuffle index\n",
        "        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
        "\n",
        "        # index for restoring\n",
        "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
        "\n",
        "        return x[idx_shuffle], idx_unshuffle\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n",
        "        \"\"\"\n",
        "        Undo batch shuffle.\n",
        "        \"\"\"\n",
        "        return x[idx_unshuffle]\n",
        "\n",
        "    # Defining contrastive loss function\n",
        "    def contrastive_loss(self, im_q, im_k):\n",
        "\n",
        "        # compute query features\n",
        "        # Feature representations of the query view from the query encoder\n",
        "        q = self.encoder_q(im_q)  # queries: NxC\n",
        "        q = nn.functional.normalize(q, dim=1)  # already normalized\n",
        "\n",
        "        # compute key features\n",
        "        # Get shuffled and reversed indexes for the current minibatch\n",
        "        with torch.no_grad():  # no gradient to keys\n",
        "            # shuffle for making use of BN\n",
        "            im_k_, idx_unshuffle = self._batch_shuffle_single_gpu(im_k)\n",
        "\n",
        "            k = self.encoder_k(im_k_)  # keys: NxC\n",
        "            k = nn.functional.normalize(k, dim=1)  # Normalize the feature representations\n",
        "\n",
        "            # undo shuffle\n",
        "            k = self._batch_unshuffle_single_gpu(k, idx_unshuffle)\n",
        "\n",
        "        # With similarity measured by dot product, a form of a contrastive loss function, called InfoNCE is used in the paper\n",
        "        # Einstein sum is more intuitive\n",
        "        # It Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.\n",
        "        # positive logits: Nx1 (Compute sim between positive views)\n",
        "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
        "        # negative logits: NxK (Compute similarity between postive and all negatives in the memory)\n",
        "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()]) # Get queue from register_buffer (self.queue.clone().detach())\n",
        "\n",
        "        # logits: Nx(1+K)\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "        # apply temperature\n",
        "        logits /= self.T\n",
        "\n",
        "        # labels: positive key indicators\n",
        "        # For the current mini-batch, we encode the queries and their corresponding keys, which form the positive sample pairs\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
        "\n",
        "        # softmax-based classifier that tries to classify q as k+\n",
        "        # A contrastive loss is a function whose value is low when query(q) is similar to its positive key k+\n",
        "        loss = nn.CrossEntropyLoss().cuda()(logits, labels)\n",
        "\n",
        "        return loss, q, k\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, im1, im2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            im_q: a batch of query images\n",
        "            im_k: a batch of key images\n",
        "        Output:\n",
        "            loss\n",
        "        \"\"\"\n",
        "\n",
        "        # update the key encoder\n",
        "        with torch.no_grad():  # no gradient to keys\n",
        "            self._momentum_update_key_encoder()\n",
        "\n",
        "        # compute loss\n",
        "        if self.symmetric:  # asymmetric loss\n",
        "            loss_12, q1, k2 = self.contrastive_loss(im1, im2)\n",
        "            loss_21, q2, k1 = self.contrastive_loss(im2, im1)\n",
        "            loss = loss_12 + loss_21\n",
        "            k = torch.cat([k1, k2], dim=0)\n",
        "        else:  # asymmetric loss\n",
        "            loss, q, k = self.contrastive_loss(im1, im2)\n",
        "\n",
        "        # Update the queue/memory with the current key_encoder minibatch.\n",
        "        self._dequeue_and_enqueue(k)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the model"
      ],
      "metadata": {
        "id": "cwJrvcnOM-5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moco_dim = 128 # feature dimension\n",
        "moco_k = 4096  # queue size; number of negative keys\n",
        "moco_m = 0.99  # moco momentum of updating key encoder\n",
        "moco_t = 0.1   # softmax temperature\n",
        "bn_splits = 8  # simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu\n",
        "symmetric = False # use a symmetric loss function that backprops to both crops\n",
        "\n",
        "# Create model\n",
        "model = ModelMoCo(\n",
        "        dim=moco_dim,\n",
        "        K=moco_k,\n",
        "        m=moco_m,\n",
        "        T=moco_t,\n",
        "        bn_splits=bn_splits,\n",
        "        symmetric=symmetric,\n",
        "    ).cuda()\n",
        "print(model.encoder_q)"
      ],
      "metadata": {
        "id": "iAABmlP45q3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YXcpXBwi8KV"
      },
      "source": [
        "### Define training function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBKFqboqnqty"
      },
      "outputs": [],
      "source": [
        "# Train for one epoch\n",
        "def train(net, data_loader, train_optimizer, epoch):\n",
        "    # Set the model in training mode\n",
        "    net.train()\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
        "\n",
        "    # load a minibatch x with N samples\n",
        "    for im_1, im_2 in train_bar:\n",
        "        # im_1 : randomly augmented image\n",
        "        # im_2 : another randomly augmented image\n",
        "        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
        "\n",
        "        # Forward pass of the model\n",
        "        loss = net(im_1, im_2)\n",
        "\n",
        "        # Zero out the gradients\n",
        "        train_optimizer.zero_grad()\n",
        "\n",
        "        # SGD update: query network\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        train_optimizer.step()\n",
        "\n",
        "        total_num += data_loader.batch_size\n",
        "        total_loss += loss.item() * data_loader.batch_size\n",
        "        train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, epochs, optimizer.param_groups[0]['lr'], total_loss / total_num))\n",
        "\n",
        "    return total_loss / total_num\n",
        "\n",
        "# lr scheduler for training\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    lr = 0.06\n",
        "    if cos:  # cosine lr schedule\n",
        "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
        "    else:  # stepwise lr schedule\n",
        "        for milestone in schedule:\n",
        "            lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define test function\n",
        "\n",
        "* After training the model we get the feature representation for the images (which we are calling as pre-trained features in this case) which can be transferrable.\n",
        "\n",
        "* Now, by using these pre-trained features we kind of apply transfer learning to perform the supervised learning\n",
        "\n",
        "* We will be extracting the features from the memory set (memory_data_loader) with out calculating the gradients.\n",
        "\n",
        "* Then we will loop through the test data to predict the label by weighted knn search.\n"
      ],
      "metadata": {
        "id": "OA_ZtvJlLKOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test using a knn monitor\n",
        "# test for one epoch, use weighted knn to find the most similar image's label to assign the test image\n",
        "# knn_k = k in kNN monitor, knn_t = softmax temperature in kNN monitor; could be different with moco_t\n",
        "def test(net, memory_data_loader, test_data_loader, epoch, knn_k = 200, knn_t = 0.1):\n",
        "\n",
        "    # Set the model mode to evaluation\n",
        "    net.eval()\n",
        "\n",
        "    # Loading the number of classes from the memory data loader\n",
        "    classes = len(memory_data_loader.dataset.classes)\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "\n",
        "    with torch.no_grad(): # not calculate the gradients\n",
        "        # Generate feature bank\n",
        "        # Loop through the batchs of images\n",
        "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
        "            feature = net(data.cuda(non_blocking=True)) # -> output shape: [512, 128] -> [batch_size, feature dimesion]\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "            feature_bank.append(feature) # Appending all the features in batches to the feature bank\n",
        "        # [D, N]\n",
        "        # contiguous(), actually makes a copy of the tensor such that the order of its elements\n",
        "        # in memory is the same as if it had been created from scratch with the same data.\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous() # feature_bank (shape[128, 50000]) -> [feature_dimension, train_images]\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device) # feature_labels (50000)\n",
        "\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            feature = net(data)\n",
        "\n",
        "            # Normalize the features\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "\n",
        "            # Call the knn-predict funcion to get kNN predictions on features based on a feature bank\n",
        "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
        "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, epochs, total_top1 / total_num * 100))\n",
        "\n",
        "    return total_top1 / total_num * 100"
      ],
      "metadata": {
        "id": "o3VVI_LTMTYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to run kNN predictions"
      ],
      "metadata": {
        "id": "64W5w4q5QgPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "RI1Y8bSImD7N"
      },
      "outputs": [],
      "source": [
        "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
        "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
        "\n",
        "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k=200, knn_t=0.1):\n",
        "\n",
        "    \"\"\"Helper function to run kNN predictions on features based on a feature bank\n",
        "    Args:\n",
        "        feature: Tensor of shape [N, D] consisting of N D-dimensional features\n",
        "        feature_bank: Tensor of a database of features used for kNN\n",
        "        feature_labels: Labels for the features in our feature_bank\n",
        "        classes: Number of classes (e.g. 10 for CIFAR-10)\n",
        "        knn_k: Number of k neighbors used for kNN\n",
        "        knn_t: 0.1\n",
        "    \"\"\"\n",
        "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "    # Performs a matrix multiplication of the matrices input and mat2.\n",
        "    # If input is a (n×m) tensor, mat2 is a (m×p) tensor, out will be a (n×p) tensor\n",
        "    sim_matrix = torch.mm(feature, feature_bank)\n",
        "    # [B, K]\n",
        "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
        "    # [B, K]\n",
        "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
        "    # we do a reweighting of the similarities\n",
        "    sim_weight = (sim_weight / knn_t).exp()\n",
        "\n",
        "    # counts for each class\n",
        "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
        "    # [B*K, C]\n",
        "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "    # weighted score ---> [B, C]\n",
        "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86lHkiKox3KO"
      },
      "source": [
        "### Start training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.06\n",
        "epochs = 20  # number of total epochs\n",
        "wd = 0.0005  # weight decay\n",
        "cos = True  # learning rate schedule (when to drop lr by 10x); does not take effect if cos is on\n",
        "schedule = []\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)"
      ],
      "metadata": {
        "id": "Vqay8EX05InT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The model is already trained for 200 epochs and saved the checkpoint (model, optimizer). Now, we will be running it only for 20 epochs."
      ],
      "metadata": {
        "id": "ZVV0dPwAQ0Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the downloaded checkpoint file path\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath) # Load the saved or downloaded checkpoint\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "ej7oHonv4d1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the checkpoint path here\n",
        "ckp_path = \"/content/MoCo_model_checkpoint.pth\"\n",
        "\n",
        "# Call the load checkpoint function by passing the file path\n",
        "# Basically, you first initialize your model and optimizer and then update the state dictionaries using the load checkpoint function.\n",
        "model, optimizer = load_ckp(ckp_path, model, optimizer)"
      ],
      "metadata": {
        "id": "CI4lTqOP4kGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir outputs"
      ],
      "metadata": {
        "id": "o6djU-7MhSBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3INyoTC3-bf"
      },
      "outputs": [],
      "source": [
        "# logging\n",
        "results = {'train_loss': [], 'test_accuracy': []}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Call train function\n",
        "    train_loss = train(model, train_loader, optimizer, epoch)\n",
        "    results['train_loss'].append(train_loss)\n",
        "\n",
        "    # Call test function\n",
        "    test_acc = test(model.encoder_q, memory_loader, test_loader, epoch)\n",
        "    results['test_accuracy'].append(test_acc)\n",
        "\n",
        "    # Save model\n",
        "    torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),},  '/content/outputs/model_last.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mIPZ4gnOcgnN"
      },
      "source": [
        "#@title Q.1. Contrastive learning is a way of building a discrete dictionary on high-dimensional continuous inputs such as images. The dictionary is dynamic in the sense that the keys are randomly sampled, and that the key encoder evolves during training.\n",
        "Answer1 = \"TRUE\" #@param [\"\",\"TRUE\", \"FALSE\"]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNaLFvpbcgm5"
      },
      "source": [
        "#### Consider the following statements about MoCo and answer Q2.\n",
        "\n",
        "\n",
        "A. Momentum Contrast (MoCo) trains a visual representation encoder by matching an encoded query q to a dictionary\n",
        "of encoded keys using a contrastive loss\n",
        "\n",
        "B. The dictionary is built as a queue, the encoded representations of the current mini-batch are enqueued,\n",
        "and the oldest are dequeued, decoupling it from the mini-batch size allowing it to be large.\n",
        "\n",
        "C. The keys are encoded by a slowly progressing key encoder, driven by a momentum-based moving average with the query\n",
        "encoder, which enables a large and consistent dictionaries for learning visual representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_F3RfGojqZDc"
      },
      "source": [
        "#@title Q.2. Which of the above statements is/are True for Momentum Contrast (MOCO)?\n",
        "Answer2 = \"A, B and C\" #@param [\"\",\"Only A\", \"Only C\", \"Only A and B\", \"Only B and C\", \"Only A and C\", \"A, B and C\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "a1a65927-3e60-4489-a49b-d3872f556f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 3022\n",
            "Date of submission:  13 Nov 2023\n",
            "Time of submission:  12:46:17\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}