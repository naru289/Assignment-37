{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-37/blob/main/Domain_adaptation_on_PACS_dataset(Ungraded).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment: Implementation of Domain Adaptation Aglorithm on PACS Dataset"
      ],
      "metadata": {
        "id": "eBVdbYrsUdqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n"
      ],
      "metadata": {
        "id": "nXph6B6IMc8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field of Computer vision has made significant progress thanks to effectiveness of Convolutional Neural Networks (CNNs). As an example, in a classification task, you would often use one of the standard network architectures out there (AlexNet, ResNet, etc.) and train it using your dataset. This would likely lead to very good performance. Moreover, using Transfer Learning and training on a network that is pretrained on a large dataset and tune some of its upper layers using your own small annotated dataset has a huge impact on the results. Both of these approaches assume that your training data is representative of the underlying distribution. However, if the inputs at test time differ significantly from the training data, the model might not perform very well.\n",
        "\n",
        "\n",
        "The reason the model did not perform very well in these scenarios is that the problem domain  changed. In this particular case, the domain of the input data changed while the task domain (the  labels) remained the same. On other occasions, you may want to use data from the same domain  to accomplish a new task. Similarly, the input and task domains could differ at the same time. In  these cases, domain adaptation comes to aid. Domain adaptation is a sub-discipline of machine  learning which deals with scenarios in which a model trained on a source distribution is used in  the context of a different target distribution. In general, domain adaptation uses labeled data in  one or more source domains to solve new tasks in a target domain.\n",
        "\n",
        "Our method is to use Adversarial Domain Adaptation. This technique tries to achieve domain  adaptation by using adversarial training. if we use the domain confusion loss in addition to the  classification loss used for the current task. The domain confusion loss is similar to the  discriminator in GANs in that it tries to match the distributions of the source and target domains  in order to “confuse” the high-level classification layers. Perhaps the most famous example of  such a network is the Domain-Adversarial Neural Network (DANN). This network consists of two losses, the classification loss and the domain confusion loss. It contains a gradient reversal layer to  match the feature distributions. By minimizing the classification loss for the source samples and  the domain confusion loss for all samples, this makes sure that the samples are mutually  indistinguishable for the classifiers.\n",
        "\n"
      ],
      "metadata": {
        "id": "YTCwFaYAMm5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Domain adaptation** is the ability to apply an algorithm trained in one or more \"source domains\" to a different (but related) \"target domain\". Domain adaptation is a subcategory of transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different target data distribution.\n",
        "\n",
        "In order to tackle the issue, a modified version of the AlexNet is used allowing not only to classify input images in the source domain but also to transfer this capability to the target domain.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/hadinej/DL-Domain-adaptation-on-PACS-dataset/raw/main/images/dann_architecture.jpg\" width=650px/>\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQhGxxzqNS5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "For this anaysis the PACS dataset is used. It contains overall 9991 images, splittd unevenly among:\n",
        "\n",
        "* 7 classes (Dog, Elephant, Giraffe, Guitar, Horse, House, Person) and\n",
        "* 4 domains: Art painting, Cartoon, Photo and Sketch.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/hadinej/DL-Domain-adaptation-on-PACS-dataset/raw/main/images/example_PACSdata_horse.jpg\" width=650px/>\n",
        "\n",
        "Sample images from the PACS dataset for the class 'horse' one for each domain, from left to right photo, art painting, cartoon and sketch.\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "d70Kt5QkNjLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "GNd9ZTJwUmvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MachineLearning2020/Homework3-PACS.git"
      ],
      "metadata": {
        "id": "remiGpqJirwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages"
      ],
      "metadata": {
        "id": "KhXOoVY7UqGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJzl8EhuiE1F"
      },
      "source": [
        "### Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = 7\n",
        "DIR_RUNS = '02 VERE PROVE/'\n",
        "\n",
        "# -----------------\n",
        "ID_RUN = '9'\n",
        "RUN_NUMBER = '1'\n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "\t\t\t\t\t\t\t\t\t\t # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-2          # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 60      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 30        # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "ALPHA = 0.1\t\t\t\t\t   # Multiplicative factor for the backpropogating gradient from the domain_classifier\n",
        "\n",
        "DOMAIN_ADAPTATION = True\n",
        "EVAL_ACCURACY_ON_TRAINING = True\n",
        "EVAL_LOSS_AND_ACCURACY_ON_VALIDATION = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "### Define Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "outputs": [],
      "source": [
        "MEANS, STDS = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "\n",
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([#transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "\t\t\t\t\t\t\t\t\ttransforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # Remember this when applying different transformations, otherwise you get an error\n",
        "\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\ttransforms.Normalize(MEANS, STDS) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([#transforms.Resize(256),\n",
        "\t\t\t\t\t\t\t\t\ttransforms.CenterCrop(224),\n",
        "\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\ttransforms.Normalize(MEANS, STDS)\n",
        "])\n",
        "\n",
        "DATA_DIR = 'Homework3-PACS/PACS'\n",
        "\n",
        "photo_dataset = torchvision.datasets.ImageFolder(DATA_DIR+'/photo', transform=train_transform)\n",
        "art_dataset = torchvision.datasets.ImageFolder(DATA_DIR+'/art_painting', transform=train_transform)\n",
        "cartoon_dataset = torchvision.datasets.ImageFolder(DATA_DIR+'/cartoon', transform=eval_transform)\n",
        "sketch_dataset = torchvision.datasets.ImageFolder(DATA_DIR+'/sketch', transform=eval_transform)\n",
        "\n",
        "PACSdataset = [photo_dataset, art_dataset, cartoon_dataset, sketch_dataset]\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Photo Dataset: {}'.format(len(photo_dataset)))\n",
        "print('Art Dataset: {}'.format(len(art_dataset)))\n",
        "print('Cartoon Dataset: {}'.format(len(cartoon_dataset)))\n",
        "print('Sketch Dataset: {}'.format(len(sketch_dataset)))\n",
        "\n",
        "print('Classes:', photo_dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for plotting the distribution of classes across domain"
      ],
      "metadata": {
        "id": "CFiD9qgTVlDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "def plotBar(data1, data2, data3, data4, classes, color='#3949AB'):\n",
        "\t# unique, counts = np.unique(data, return_counts=True)\n",
        "\t# print('Mean images per class:', counts.mean())\n",
        "\t# print('Std images per class:', counts.std())\n",
        "\n",
        "\tunique, counts1 = np.unique(data1, return_counts=True)\n",
        "\tunique, counts2 = np.unique(data2, return_counts=True)\n",
        "\tunique, counts3 = np.unique(data3, return_counts=True)\n",
        "\tunique, counts4 = np.unique(data4, return_counts=True)\n",
        "\n",
        "\tfig, ax = plt.subplots(nrows=1, ncols=1)\n",
        "\tax.set_title('Distribution of classes across domains', pad=20.0, alpha=0.85, fontweight='bold')\n",
        "\n",
        "\twidth=0.18\n",
        "\n",
        "\t# plt.bar(unique, counts, width=width, color=color)\n",
        "\tplt.bar(unique-2*(width)+(width/2), counts1, width=width, color='#3949AB70', edgecolor='#3949AB95', linewidth=0.5, label='Photo')\n",
        "\tplt.bar(unique-(width/2), counts2, width=width, color='#f4433670', edgecolor='#f4433695', linewidth=0.5, label='Art paintings')\n",
        "\tplt.bar(unique+(width/2), counts3, width=width, color='#388E3C70', edgecolor='#388E3C95', linewidth=0.5, label='Cartoon')\n",
        "\tplt.bar(unique+2*(width)-(width/2), counts4, width=width, color='#FF8F0070', edgecolor='#FF8F0095', linewidth=0.5, label='Sketch')\n",
        "\n",
        "\tplt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
        "\n",
        "\tax.set_xticks(unique)\n",
        "\tax.set_xticklabels([(classe[0].upper()+classe[1:]) for classe in classes])\n",
        "\n",
        "\tplt.grid(alpha=0.2, axis='y')\n",
        "\n",
        "\tax.legend()\n",
        "\tplt.show()\n",
        "\n",
        "\n",
        "def imgshow(img):\n",
        "\timg = img / 2 + 0.5     # unnormalize\n",
        "\tnpimg = img.numpy()\n",
        "\tplt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\tplt.show()\n",
        "\n",
        "\treturn"
      ],
      "metadata": {
        "id": "_1Q2ZbRODAL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of images from the dataset"
      ],
      "metadata": {
        "id": "laexHflbVcHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Print some images\n",
        "nPhotos = 4\n",
        "for dataset in PACSdataset:\n",
        "\tfor i in range(nPhotos):\n",
        "\t\tindex = np.random.randint(0, len(dataset))\n",
        "\t\timgshow(dataset[index][0])\n",
        "\n",
        "# --- Explore class distributions across domains\n",
        "labels_photo = photo_dataset.targets\n",
        "labels_art = art_dataset.targets\n",
        "labels_cartoon = cartoon_dataset.targets\n",
        "labels_sketch = sketch_dataset.targets\n",
        "plotBar(labels_photo, labels_art, labels_cartoon, labels_sketch, photo_dataset.classes)"
      ],
      "metadata": {
        "id": "edkA-cFVDDxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training dataloaders\n",
        "photo_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "art_dataloader = DataLoader(art_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "# --- Test datasets\n",
        "photo_test_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "art_test_dataloader = DataLoader(art_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "cartoon_test_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "sketch_test_dataloader = DataLoader(sketch_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "kagcfvVqCFS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Model (Reconstructed AlexNet Model):\n",
        "\n",
        "our task is to forge our model from AlexNet by first pre-loading the pre-trained weights on  ImageNet and modifying the “init function” of the module and add the Domain Classifier.  Considering the structure of DANN, I created a custom module where it can be seen in the  following figure, that I separated the layers of original AlexNet and first created my feature  extractor which are the first 5 convolutional layers of the model. The importance of this step is  that I need to pre-load the weights by using “state_dict”.\n",
        "\n",
        "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are  contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply  a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with  learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnorm’s  running_mean) have entries in the model’s state_dict. Optimizer objects (torch.optim) also have  a state_dict, which contains information about the optimizer’s state, as well as the  hyperparameters used.\n",
        "\n",
        "Following that we use the same Fully connected layer of the original model in parallel for both  our Label Predictor and Domain Classifier. Now for introducing our DANN task we must modify  the forward function of the model and apply the principles of Adversarial Domain Adaptation.  Where we define the “ReverseLayerF” to apply the gradient reversal layer."
      ],
      "metadata": {
        "id": "nV0tItZVVTfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    # Forwards identity\n",
        "    # Sends backward reversed gradients\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class AlexNetDANN(nn.Module):\n",
        "\n",
        "\tdef __init__(self, num_classes=1000):\n",
        "\t\tsuper(AlexNetDANN, self).__init__()\n",
        "\n",
        "\t\tself.features = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\t\t\tnn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\t\t\tnn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\t\t)\n",
        "\t\tself.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "\t\tself.classifier = nn.Sequential(\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(256 * 6 * 6, 4096),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(4096, 4096),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Linear(4096, num_classes),\n",
        "\t\t)\n",
        "\n",
        "\t\tself.domain_classifier = nn.Sequential(\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(256 * 6 * 6, 4096),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(4096, 4096),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Linear(4096, 2),\n",
        "\t\t)\n",
        "\n",
        "\n",
        "\t# DEFINE HOW FORWARD PASS IS COMPUTED\n",
        "\tdef forward(self, x, alpha=None, dest='classifier'):\n",
        "\t\t\"\"\"\n",
        "\t\t# PYTORCH ALEXNET IMPLEMENTATION\n",
        "\t\t\tx = self.features(x)\n",
        "\t\t\tx = self.avgpool(x)\n",
        "\t\t\tx = torch.flatten(x, 1)\n",
        "\t\t\tx = self.classifier(x)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t# HW3 SUGGESTION OF IMPLEMENTATION\n",
        "\t\t\tfeatures = self.features\n",
        "\t        features = features.view(features.size(0), -1)\n",
        "\t        if alpha is not None:\n",
        "\t\t        reverse_feature = ReverseLayerF.apply(features, alpha)\n",
        "\t\t        discriminator_output = ...\n",
        "\t\t        return discriminator_output\n",
        "\t        else:\n",
        "\t            class_outputs = ...\n",
        "\t            return class_outputs\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t# PYTORCH DANN IMPLEMENTATION\n",
        "\t\t\tinput_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
        "\t        feature = self.feature(input_data)\n",
        "\t        feature = feature.view(-1, 50 * 4 * 4)\n",
        "\t        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "\t        class_output = self.class_classifier(feature)\n",
        "\t        domain_output = self.domain_classifier(reverse_feature)\n",
        "        \"\"\"\n",
        "\n",
        "\t\tx = self.features(x)\n",
        "\t\tx = self.avgpool(x)\n",
        "\t\tfeatures = torch.flatten(x, 1)\n",
        "\n",
        "\t\tif dest == 'classifier':\n",
        "\t\t\toutput = self.classifier(features)\n",
        "\t\t\treturn output\n",
        "\n",
        "\t\telif dest == 'domain_classifier':\n",
        "\t\t\tif alpha == None:\n",
        "\t\t\t\tprint('FATAL ERROR - Attach a valid alpha when forwarding to the domain classifier')\n",
        "\t\t\t\tsys.exit()\n",
        "\n",
        "\t\t\treverse_features = ReverseLayerF.apply(features, alpha)\n",
        "\t\t\tdomain_output = self.domain_classifier(reverse_features)\n",
        "\t\t\treturn domain_output\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tprint('FATAL ERROR - Invalid parameters to forward function in AlexNetDANN')\n",
        "\t\t\tsys.exit()"
      ],
      "metadata": {
        "id": "SlTcfJ2zDjIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the pretrained Alexnet model"
      ],
      "metadata": {
        "id": "sX7-A4P4wUY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alexnetDANN(pretrained=True, progress=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNetDANN(num_classes=1000, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "\t# Change output classes\n",
        "    model.classifier[6] = nn.Linear(4096, 7)\n",
        "    model.domain_classifier[6] = nn.Linear(4096, 2)\n",
        "\n",
        "    # Copy pretrained weights from the classifier to the domain_classifier\n",
        "    model.domain_classifier[1].weight.data = model.classifier[1].weight.data.clone()\n",
        "    model.domain_classifier[1].bias.data = model.classifier[1].bias.data.clone()\n",
        "\n",
        "    model.domain_classifier[4].weight.data = model.classifier[4].weight.data.clone()\n",
        "    model.domain_classifier[4].bias.data = model.classifier[4].bias.data.clone()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MfMs461xwSuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the model"
      ],
      "metadata": {
        "id": "xnXQdJ5aww2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = alexnetDANN(pretrained=True)\n",
        "\n",
        "# --- Check that pretrined weights have been correctly copied into the domain_classifier branch as well\n",
        "#for domain_param, classifier_param in zip(list(net.classifier.parameters()), list(net.domain_classifier.parameters())):\n",
        "  #print(torch.allclose(domain_param, classifier_param))"
      ],
      "metadata": {
        "id": "qNNiUUYPDYMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "metadata": {
        "id": "-QK1T_W_DiS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "SOVdg9KXw1QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "criterion_val = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# Choose parameters to optimize\n",
        "parameters_to_optimize = net.parameters()\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "metadata": {
        "id": "6bFLAa2NEqQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluating the model"
      ],
      "metadata": {
        "id": "r8Eg97mXw6Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = net.to(DEVICE)\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "losses_train = []\n",
        "losses_train_classifier = []\n",
        "losses_train_domain = []\n",
        "losses_val = []\n",
        "accuracies_val = []\n",
        "accuracies_train = []\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  if DOMAIN_ADAPTATION:\n",
        "    len_dataloader = min(len(photo_dataloader), len(art_dataloader))\n",
        "    data_source_iter = iter(photo_dataloader)\n",
        "    data_target_iter = iter(art_dataloader)\n",
        "  else:\n",
        "    len_dataloader = len(photo_dataloader)\n",
        "    data_source_iter = iter(photo_dataloader)\n",
        "\n",
        "  i = 0\n",
        "  while i < len_dataloader:\n",
        "  #for images, labels in photo_dataloader:\n",
        "\t  # Bring data over the device of choice\n",
        "    data_source = next(data_source_iter)\n",
        "    s_images, s_labels = data_source\n",
        "\n",
        "    #print('Batch size inferred 1:', len(s_labels))\n",
        "\n",
        "    s_images = s_images.to(DEVICE)\n",
        "    s_labels = s_labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    if DOMAIN_ADAPTATION:\n",
        "      # FEED-FORWARD SOURCE TO CLASSIFIER\n",
        "      source_classifier_outputs = net(s_images, dest='classifier')\n",
        "      source_classifier_err = criterion(source_classifier_outputs, s_labels)\n",
        "\n",
        "      # FEED-FORWARD SOURCE TO DOMAIN_CLASSIFIER\n",
        "      source_domain_outputs = net(s_images, alpha=ALPHA, dest='domain_classifier')\n",
        "      source_domain_labels = torch.zeros(BATCH_SIZE).long().to(DEVICE)\n",
        "      source_domain_err = criterion(source_domain_outputs, source_domain_labels)\n",
        "\n",
        "      # FEED-FORWARD TARGET TO DOMAIN_CLASSIFIER\n",
        "      data_target = next(data_target_iter)\n",
        "      t_images, t_labels = data_target\n",
        "\n",
        "      #print('Batch size inferred 2:', len(t_labels))\n",
        "\n",
        "      t_images = t_images.to(DEVICE)\n",
        "      t_labels = t_labels.to(DEVICE)\n",
        "\n",
        "      target_domain_outputs = net(t_images, alpha=ALPHA, dest='domain_classifier')\n",
        "      target_domain_labels = torch.ones(BATCH_SIZE).long().to(DEVICE)\n",
        "      target_domain_err = criterion(target_domain_outputs, target_domain_labels)\n",
        "\n",
        "      loss = source_classifier_err + source_domain_err + target_domain_err\n",
        "\n",
        "    else:\n",
        "      # Forward pass to the network\n",
        "      outputs = net(s_images, dest='classifier')\n",
        "      # Compute loss based on output and ground truth\n",
        "      loss = criterion(outputs, s_labels)\n",
        "\n",
        "    if current_step == 0 and DOMAIN_ADAPTATION == False:\n",
        "      print('--- Initial loss on train: {}'.format(loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "    i +=1\n",
        "\n",
        "\t# End of one epoch\n",
        "  if DOMAIN_ADAPTATION:\n",
        "    print('--- Epoch {}'.format(epoch+1))\n",
        "    losses_train_classifier.append(source_classifier_err.item())\n",
        "    losses_train_domain.append((source_domain_err.item()+target_domain_err.item()))\n",
        "  else:\n",
        "    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "  if EVAL_LOSS_AND_ACCURACY_ON_VALIDATION:\n",
        "\n",
        "    net.train(False)\n",
        "\n",
        "    running_corrects_val = 0\n",
        "    cum_loss_val = 0\n",
        "\n",
        "    #for images_val, labels_val in tqdm(val_dataloader):\n",
        "    for images_val, labels_val in art_test_dataloader:\n",
        "      images_val = images_val.to(DEVICE)\n",
        "      labels_val = labels_val.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs_val = net(images_val, dest='classifier')\n",
        "\n",
        "      cum_loss_val += criterion_val(outputs_val, labels_val).item()\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs_val.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects_val += torch.sum(preds == labels_val.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy_val = running_corrects_val / float(len(art_dataset))\n",
        "    loss_val = cum_loss_val / float(len(art_dataset))\n",
        "\n",
        "    losses_val.append(loss_val)\n",
        "    accuracies_val.append(accuracy_val)\n",
        "\n",
        "    print('Loss on val:', loss_val)\n",
        "    print('Accuracy on val:', accuracy_val)\n",
        "\n",
        "  if EVAL_ACCURACY_ON_TRAINING:\n",
        "    net.train(False)\n",
        "\n",
        "    running_corrects_train = 0\n",
        "\n",
        "    for images_train, labels_train in photo_test_dataloader:\n",
        "      images_train = images_train.to(DEVICE)\n",
        "      labels_train = labels_train.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs_train = net(images_train, dest='classifier')\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs_train.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects_train += torch.sum(preds == labels_train.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy_train = running_corrects_train / float(len(photo_dataset))\n",
        "\n",
        "    accuracies_train.append(accuracy_train)\n",
        "\n",
        "    print('Accuracy on train:', accuracy_train)\n",
        "\n",
        "\t# Step the scheduler\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "byoMk70qDiWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = net.to(DEVICE)\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(art_test_dataloader):\n",
        "\timages = images.to(DEVICE)\n",
        "\tlabels = labels.to(DEVICE)\n",
        "\n",
        "\t# Forward Pass\n",
        "\toutputs = net(images, dest='classifier')\n",
        "\n",
        "\t# Get predictions\n",
        "\t_, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "\t# Update Corrects\n",
        "\trunning_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(art_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "metadata": {
        "id": "j9vPFU_zM0xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}